var documenterSearchIndex = {"docs":
[{"location":"library_api/sites/","page":"Traces, choices, and call sites","title":"Traces, choices, and call sites","text":"CurrentModule = Jaynes","category":"page"},{"location":"library_api/sites/","page":"Traces, choices, and call sites","title":"Traces, choices, and call sites","text":"Trace","category":"page"},{"location":"library_api/sites/#Jaynes.Trace","page":"Traces, choices, and call sites","title":"Jaynes.Trace","text":"abstract type Trace end\n\nAbstract base type of all traces.\n\n\n\n\n\n","category":"function"},{"location":"library_api/sites/","page":"Traces, choices, and call sites","title":"Traces, choices, and call sites","text":"HierarchicalTrace","category":"page"},{"location":"library_api/sites/","page":"Traces, choices, and call sites","title":"Traces, choices, and call sites","text":"VectorizedTrace","category":"page"},{"location":"library_api/sites/","page":"Traces, choices, and call sites","title":"Traces, choices, and call sites","text":"ChoiceSite","category":"page"},{"location":"library_api/sites/","page":"Traces, choices, and call sites","title":"Traces, choices, and call sites","text":"HierarchicalCallSite","category":"page"},{"location":"library_api/sites/","page":"Traces, choices, and call sites","title":"Traces, choices, and call sites","text":"VectorizedCallSite","category":"page"},{"location":"inference/pf/","page":"Particle filtering","title":"Particle filtering","text":"CurrentModule = Jaynes","category":"page"},{"location":"inference/pf/","page":"Particle filtering","title":"Particle filtering","text":"initialize_filter\nfilter_step!","category":"page"},{"location":"inference/pf/#Jaynes.initialize_filter","page":"Particle filtering","title":"Jaynes.initialize_filter","text":"particles = initialize_filter(observations::ConstrainedHierarchicalSelection,\n                              num_particles::Int,\n                              fn::Function, \n                              args::Tuple)\n\nInstantiate a set of particles using a call to importance_sampling.\n\n\n\n\n\n","category":"function"},{"location":"inference/pf/#Jaynes.filter_step!","page":"Particle filtering","title":"Jaynes.filter_step!","text":"filter_step!(observations::ConstrainedHierarchicalSelection,\n             ps::Particles,\n             new_args::Tuple)\n\nPerform a single filter step from an instance ps of Particles, applying the constraints specified by observations.\n\nfilter_step!(observations::ConstrainedHierarchicalSelection,\n             ps::Particles,\n             new_args::Tuple,\n             proposal::Function,\n             proposal_args::Tuple)\n\nPerform a single filter step using a custom proposal function, applying the constraints specified by observations.\n\n\n\n\n\n","category":"function"},{"location":"inference/pf/","page":"Particle filtering","title":"Particle filtering","text":"info: Info\nCustom proposals provided to filter_step! should accept as first argument a CallSite instance (e.g. so that you can use the previous trace and information in your transition proposal).","category":"page"},{"location":"inference/pf/","page":"Particle filtering","title":"Particle filtering","text":"check_ess_resample!","category":"page"},{"location":"inference/pf/#Jaynes.check_ess_resample!","page":"Particle filtering","title":"Jaynes.check_ess_resample!","text":"check_ess_resample!(ps::Particles)\n\nChecks the effective sample size using ess, then resamples from an existing instance of Particles by mutation in place.\n\n\n\n\n\n","category":"function"},{"location":"benchmarks/","page":"-","title":"-","text":"This page contains a set of benchmarks comparing Jaynes to other probabilistic programming systems. The code for each of these benchmarks is available here.","category":"page"},{"location":"benchmarks/","page":"-","title":"-","text":"warning: Warning\nJaynes is currently in open alpha, which means that these benchmarks should not be taken seriously (any benchmarks between systems must always be taken with a grain of salt anyways) until the system is fully tested.Furthermore, I'm not in the business of inflating the performance characteristics of systems I build. This means:If you notice that I'm not testing Jaynes against optimized programs in other systems, please let me know so that I can perform accurate comparisons.\nIf you suspect that there's an issue or bug with Jaynes, please open an issue.\nIf you'd like to perform a benchmark, also please open an issue.Benchmarking is an inherently sensitive topic - I'd like to make these as fair and open as possible, so don't hesitate to reach out.","category":"page"},{"location":"benchmarks/#Particle-filtering-in-hidden-Markov-models","page":"-","title":"Particle filtering in hidden Markov models","text":"","category":"section"},{"location":"benchmarks/","page":"-","title":"-","text":"This benchmark is a single-shot time comparison between Gen and Jaynes on a single thread. The horizontal axis is number of particle filter steps (with resampling). The vertical axis is time in seconds.","category":"page"},{"location":"benchmarks/","page":"-","title":"-","text":"<div style=\"text-align:center\">\n    <img src=\"../images/benchmark_hmmpf_gen_singlethread.png\" alt=\"\" width=\"70%\"/>\n</div>","category":"page"},{"location":"benchmarks/","page":"-","title":"-","text":"This benchmark is a single-shot time comparison between Gen and Jaynes with adaptive multi-threading in Jaynes.filter_step! and Jaynes.importance_sampling!. This benchmark was executed with JULIA_NUM_THREADS=4.","category":"page"},{"location":"benchmarks/","page":"-","title":"-","text":"<div style=\"text-align:center\">\n    <img src=\"../images/benchmark_hmmpf_gen_multithread.png\" alt=\"\" width=\"70%\"/>\n</div>","category":"page"},{"location":"benchmarks/","page":"-","title":"-","text":"The bottom plots are the estimated log marginal likelihood of the data.","category":"page"},{"location":"library_api/selection_interface/","page":"Selection interface","title":"Selection interface","text":"Jaynes features an extensive Selection query language for addressing sources of randomness. The ability to constrain random choices, compute proposals for random choices in MCMC kernels, as well as gradients requires a solid set of interfaces for selecting addresses in rand calls in your program. Here, we present the main interfaces which you are likely to use. This set of interfaces specifies a sort of query language so you'll find common operations like union, intersection, etc which you can use to flexibly combined selection queries for use in your inference programs and modeling.","category":"page"},{"location":"library_api/selection_interface/#Basic-selections","page":"Selection interface","title":"Basic selections","text":"","category":"section"},{"location":"library_api/selection_interface/","page":"Selection interface","title":"Selection interface","text":"These are the basic set of Selection APIs which allow the user to query and observe throughout the call stack of the program.","category":"page"},{"location":"library_api/selection_interface/","page":"Selection interface","title":"Selection interface","text":"selection\nanywhere","category":"page"},{"location":"library_api/selection_interface/#Compositions-of-selections","page":"Selection interface","title":"Compositions of selections","text":"","category":"section"},{"location":"library_api/selection_interface/","page":"Selection interface","title":"Selection interface","text":"The selections produced by the above APIs can be combined compositionally to form more complex selections. Some of these compositions are only available for subtypes of UnconstrainedSelection.","category":"page"},{"location":"library_api/selection_interface/#Selection-utilities","page":"Selection interface","title":"Selection utilities","text":"","category":"section"},{"location":"library_api/selection_interface/","page":"Selection interface","title":"Selection interface","text":"There are a number of useful utility functions defined on subtypes of Selection. Many of these utilities are used internally - here are a few which may be of interest to the user.","category":"page"},{"location":"inference/is/","page":"Importance sampling","title":"Importance sampling","text":"CurrentModule = Jaynes","category":"page"},{"location":"inference/is/","page":"Importance sampling","title":"Importance sampling","text":"importance_sampling","category":"page"},{"location":"inference/is/#Jaynes.importance_sampling","page":"Importance sampling","title":"Jaynes.importance_sampling","text":"Samples from the model prior.\n\nparticles, normalized_weights = importance_sampling(observations::ConstrainedSelection,\n                                                    num_samples::Int,\n                                                    model::Function, \n                                                    args::Tuple)\n\nSamples from a programmer-provided proposal function.\n\nparticles, normalized_weights = importance_sampling(observations::ConstrainedSelection,\n                                                    num_samples::Int,\n                                                    model::Function, \n                                                    args::Tuple, \n                                                    proposal::Function, \n                                                    proposal_args::Tuple)\n\nRun importance sampling on the posterior over unconstrained addresses and values. Returns an instance of Particles and normalized weights.\n\n\n\n\n\n","category":"function"},{"location":"inference/is/","page":"Importance sampling","title":"Importance sampling","text":"info: Info\nAddressed randomness in a custom proposal passed to importance_sampling should satisfy the following criteria to ensure that inference is mathematically valid:Custom proposals should only propose to unobserved addresses in the original program.\nCustom proposals should not propose to addresses which do not occur in the original program.","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"CurrentModule = Jaynes","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"Jaynes supports Zygote-based reverse mode gradient computation of learnable parameters and primitive probabilistic choices. This functionality is accessed through two different gradient contexts.","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"ParameterBackpropagateContext","category":"page"},{"location":"library_api/diff_prog/#Jaynes.ParameterBackpropagateContext","page":"Differentiable programming","title":"Jaynes.ParameterBackpropagateContext","text":"mutable struct ParameterBackpropagateContext{T <: Trace} <: BackpropagationContext\n    tr::T\n    weight::Float64\n    initial_params::AddressMap\n    params::ParameterStore\n    param_grads::Gradients\nend\n\nParameterBackpropagateContext is used to compute the gradients of parameters with respect to following objective:\n\nOuter constructors:\n\nParameterBackpropagate(tr::T, params) where T <: Trace = ParameterBackpropagateContext(tr, 0.0, params, Gradients())\nParameterBackpropagate(tr::T, params, param_grads::Gradients) where {T <: Trace, K <: Target} = ParameterBackpropagateContext(tr, 0.0, params, param_grads)\n\n\n\n\n\n","category":"type"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"For one-shot gradient computations, this context is easily accessed through the get_learnable_gradients method.","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"get_learnable_gradients","category":"page"},{"location":"library_api/diff_prog/#Jaynes.get_learnable_gradients","page":"Differentiable programming","title":"Jaynes.get_learnable_gradients","text":"gradients = get_learnable_gradients(params, cl::T, ret_grad, scaler::Float64 = 1.0) where T <: CallSite\n\nReturns a Gradients object which tracks the gradients of the objective with respect to parameters in the program.\n\n\n\n\n\n","category":"function"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"For choices, the context is ChoiceBackpropagateContext.","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"ChoiceBackpropagateContext","category":"page"},{"location":"library_api/diff_prog/#Jaynes.ChoiceBackpropagateContext","page":"Differentiable programming","title":"Jaynes.ChoiceBackpropagateContext","text":"mutable struct ChoiceBackpropagateContext{T <: Trace} <: BackpropagationContext\n    tr::T\n    weight::Float64\n    initial_params::AddressMap\n    params::ParameterStore\n    param_grads::Gradients\nend\n\nChoiceBackpropagateContext is used to compute the gradients of choices with respect to following objective:\n\nOuter constructors:\n\nChoiceBackpropagate(tr::T, init_params, params, choice_grads) where {T <: Trace, K <: Target} = ChoiceBackpropagateContext(tr, 0.0, params, choice_grads, SelectAll())\nChoiceBackpropagate(tr::T, init_params, params, choice_grads, sel::K) where {T <: Trace, K <: Target} = ChoiceBackpropagateContext(tr, 0.0, params, choice_grads, sel)\n\n\n\n\n\n","category":"type"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"For one-shot gradient computations on choices, the ChoiceBackpropagateContext is easily accessed through the get_choice_gradients method.","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"get_choice_gradients","category":"page"},{"location":"library_api/diff_prog/#Jaynes.get_choice_gradients","page":"Differentiable programming","title":"Jaynes.get_choice_gradients","text":"gradients = get_choice_gradients(params, cl::T, ret_grad) where T <: CallSite\ngradients = get_choice_gradients(cl::T, ret_grad) where T <: CallSite\n\nReturns a Gradients object which tracks the gradients with respect to the objective of random choices with differentiable logpdf in the program.\n\n\n\n\n\n","category":"function"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"In the future, Jaynes will support a context which allows the automatic training of neural network components (Flux.jl or otherwise) facilitated by custom call sites. See the foreign model interface for more details.","category":"page"},{"location":"library_api/diff_prog/#Updating-parameters","page":"Differentiable programming","title":"Updating parameters","text":"","category":"section"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"As part of standard usage, it's likely that you'd like to update learnable parameters in your model (which you declare with learnable(addr, initial_value). ","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"warning: Warning\nCurrently, there's a bug with declaring learnable structures which prevents the usage of parameters of type other than Float64 or Array{Float64, 1}. If you run into Zygote errors with mutating arrays, you can try to alleviate the problem by annotating your parameters (e.g. Float64[1.0, 3.0, ...]). You'll mostly be okay if you can stick to scalar parameters and 1D arrays - I'm working to identify this issue and fix it so higher-rank tensors can also be used.","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"Given a CallSite, you can extract parameters using get_parameters","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"ret, cl, w = generate(selection((:q, -0.5)), learnable_hypers)\nparams = get_parameters(cl)","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"which produces an instance of LearnableParameters by unpacking the params field in any trace kept in the call site. LearnableParameters and Gradients are explicitly kept separate from the internal call site representation, to encourage portability, as well as non-standard optimization schemes. Jaynes links up with the API provided by Flux.Optimisers through update! - this allows you to use any of the optimisers provided by Flux to update your parameters. Given a Gradients instance, to update your parameters, just call update_learnables with your favorite optimiser","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"trained_params = update_learnables(opt, params, gradients)","category":"page"},{"location":"library_api/diff_prog/","page":"Differentiable programming","title":"Differentiable programming","text":"which produces a new instance of LearnableParameters after apply the gradient descent step. You can then pass these in as an argument to the normal context interfaces (e.g. generate, simulate, etc) to use your updated parameters.","category":"page"},{"location":"inference/vi/","page":"Automatic differentiation variational inference","title":"Automatic differentiation variational inference","text":"CurrentModule = Jaynes","category":"page"},{"location":"inference/vi/","page":"Automatic differentiation variational inference","title":"Automatic differentiation variational inference","text":"advi","category":"page"},{"location":"inference/vi/#Jaynes.advi","page":"Automatic differentiation variational inference","title":"Jaynes.advi","text":"params, elbows, call_sites =  advi(sel::K,\n                                   iters::Int,\n                                   v_mod::Function,\n                                   v_args::Tuple,\n                                   mod::Function,\n                                   args::Tuple;\n                                   opt = ADAM(),\n                                   gs_samples = 100) where K <: ConstrainedSelection\n\nGiven a selection sel, perform automatic-differentiation variational inference with a proposal model v_mod. The result is a new set of trained parameters params for the variational model, the history of ELBO estimates elbows, and the call sites calls produced by the gradient estimator computation.\n\n\n\n\n\n","category":"function"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"CurrentModule = Jaynes","category":"page"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"Here is a description of the set of \"standard\" contexts which are used frequently in modeling and inference. Gradient and foreign model contexts are discussed in Differentiable programming and Foreign model interface, respectively.","category":"page"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"GenerateContext\ngenerate","category":"page"},{"location":"library_api/contexts/#Jaynes.GenerateContext","page":"Execution contexts","title":"Jaynes.GenerateContext","text":"mutable struct GenerateContext{T <: AddressMap, K <: AddressMap, P <: Parameters} <: ExecutionContext\n     tr::T\n     target::K\n     weight::Float64\n     score::Float64\n     visited::Visitor\n     params::P\nend\n\nGenerateContext is used to generate traces, as well as record and accumulate likelihood weights given observations at addressed randomness.\n\nInner constructors:\n\nGenerateContext(tr::T, target::K) where {T <: AddressMap, K <: AddressMap} = new{T, K}(tr, target, 0.0, Visitor(), Parameters())\nGenerateContext(tr::T, target::K, params::P) where {T <: AddressMap, K <: AddressMap, P <: Parameters} = new{T, K, P}(tr, target, 0.0, Visitor(), params)\n\nOuter constructors:\n\nGenerate(target::AddressMap) = GenerateContext(AddressMap(), target)\nGenerate(target::AddressMap, params) = GenerateContext(AddressMap(), target, params)\nGenerate(tr::AddressMap, target::AddressMap) = GenerateContext(tr, target)\n\n\n\n\n\n","category":"type"},{"location":"library_api/contexts/#Jaynes.generate","page":"Execution contexts","title":"Jaynes.generate","text":"ret, cl, w = generate(target::L, fn::Function, args...; params = Parameters()) where L <: AddressMap\nret, cs, w = generate(target::L, fn::typeof(rand), d::Distribution{K}; params = Parameters()) where {L <: AddressMap, K}\nret, v_cl, w = generate(target::L, fn::typeof(markov), call::Function, len::Int, args...; params = Parameters()) where L <: AddressMap\nret, v_cl, w = generate(target::L, fn::typeof(plate), call::Function, args::Vector; params = Parameters()) where L <: AddressMap\nret, v_cl, w = generate(target::L, fn::typeof(plate), d::Distribution{K}, len::Int; params = Parameters()) where {L <: AddressMap, K}\n\ngenerate provides an API to the GenerateContext execution context. You can use this function on any of the matching signatures above - it will return the return value ret, a RecordSite instance specialized to the call, and the score/weight w computed with respect to the constraints target.\n\n\n\n\n\n","category":"function"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"SimulateContext\nsimulate","category":"page"},{"location":"library_api/contexts/#Jaynes.SimulateContext","page":"Execution contexts","title":"Jaynes.SimulateContext","text":"mutable struct SimulateContext{T <: AddressMap, P <: AddressMap} <: ExecutionContext\n    tr::T\n    visited::Visitor\n    params::P\n    SimulateContext(params) where T <: AddressMap = new{T}(AddressMap(), Visitor(), params)\nend\n\nSimulateContext is used to simulate traces without recording likelihood weights. SimulateContext can be instantiated with custom AddressMap instances, which is useful when used for gradient-based learning.\n\nInner constructors:\n\nSimulateContext(params) = new{DynamicAddressMap}(AddressMap(), Visitor(), params)\n\n\n\n\n\n","category":"type"},{"location":"library_api/contexts/#Jaynes.simulate","page":"Execution contexts","title":"Jaynes.simulate","text":"ret, cl = simulate(fn::Function, args...; params = LearnableByAddress())\nret, cl = simulate(fn::typeof(rand), d::Distribution{T}; params = LearnableByAddress()) where T\nret, v_cl = simulate(c::typeof(plate), fn::Function, args::Vector; params = LearnableByAddress()) where T\nret, v_cl = simulate(fn::typeof(plate), d::Distribution{T}, len::Int; params = LearnableByAddress()) where T\nret, v_cl = simulate(c::typeof(markov), fn::Function, len::Int, args...; params = LearnableByAddress())\n\nsimulate function provides an API to the SimulateContext execution context. You can use this function on any of the matching signatures above - it will return the return value ret, and a RecordSite instance specialized to the call. simulate is used to express unconstrained generation of a probabilistic program trace, without likelihood weight recording.\n\n\n\n\n\n","category":"function"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"ProposeContext\npropose","category":"page"},{"location":"library_api/contexts/#Jaynes.ProposeContext","page":"Execution contexts","title":"Jaynes.ProposeContext","text":"mutable struct ProposeContext{T <: AddressMap, P <: AddressMap} <: ExecutionContext\n    tr::T\n    score::Float64\n    visited::Visitor\n    params::P\nend\n\nProposeContext is used to propose traces for inference algorithms which use custom proposals. ProposeContext instances can be passed sets of AddressMap to configure the propose with parameters which have been learned by differentiable programming.\n\nInner constructors:\n\nProposeContext(tr::T) where T <: AddressMap = new{T}(tr, 0.0, AddressMap())\n\nOuter constructors:\n\nPropose() = ProposeContext(AddressMap())\n\n\n\n\n\n","category":"type"},{"location":"library_api/contexts/#Jaynes.propose","page":"Execution contexts","title":"Jaynes.propose","text":"ret, g_cl, w = propose(fn::Function, args...)\nret, cs, w = propose(fn::typeof(rand), d::Distribution{K}) where K\nret, v_cl, w = propose(fn::typeof(markov), call::Function, len::Int, args...)\nret, v_cl, w = propose(fn::typeof(plate), call::Function, args::Vector)\nret, v_cl, w = propose(fn::typeof(plate), d::Distribution{K}, len::Int) where K\n\npropose provides an API to the ProposeContext execution context. You can use this function on any of the matching signatures above - it will return the return value ret, a RecordSite instance specialized to the call, and the score w.\n\n\n\n\n\n","category":"function"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"ScoreContext\nscore","category":"page"},{"location":"library_api/contexts/#Jaynes.ScoreContext","page":"Execution contexts","title":"Jaynes.ScoreContext","text":"mutable struct ScoreContext{P <: AddressMap} <: ExecutionContext\n    select::AddressMap\n    weight::Float64\n    params::P\nend\n\nThe ScoreContext is used to score selections according to a model function. For computation in the ScoreContext to execute successfully, the select selection must provide a choice for every address visited in the model function, and the model function must allow the context to visit every constraints expressed in select.\n\nInner constructors:\n\nfunction Score(obs::Vector{Tuple{K, P}}) where {P, K <: Union{Symbol, Pair}}\n    c_sel = selection(obs)\n    new{EmptyAddressMap}(c_sel, 0.0, AddressMap())\nend\n\nOuter constructors:\n\nScoreContext(obs::K, params) where {K <: AddressMap} = new(obs, 0.0, params)\nScore(obs::Vector) = ScoreContext(selection(obs))\nScore(obs::AddressMap) = ScoreContext(obs, AddressMap())\nScore(obs::AddressMap, params) = ScoreContext(obs, params)\n\n\n\n\n\n","category":"type"},{"location":"library_api/contexts/#Jaynes.score","page":"Execution contexts","title":"Jaynes.score","text":"ret, w = score(sel::L, fn::Function, args...; params = AddressMap()) where L <: AddressMap\nret, w = score(sel::L, fn::typeof(rand), d::Distribution{K}; params = AddressMap()) where {L <: AddressMap, K}\nret, w = score(sel::L, fn::typeof(markov), call::Function, len::Int, args...; params = AddressMap()) where L <: AddressMap\nret, w = score(sel::L, fn::typeof(plate), call::Function, args::Vector; params = AddressMap()) where L <: AddressMap\nret, w = score(sel::L, fn::typeof(plate), d::Distribution{K}, len::Int; params = AddressMap()) where {L <: AddressMap, K}\n\nscore provides an API to the ScoreContext execution context. You can use this function on any of the matching signatures above - it will return the return value ret, and the likelihood weight score of the user-provided selection sel. The selection should satisfy the following requirement:\n\nAt any random choice in any branch traveled according to the constraints of sel, sel must provide a constraint for that choice.\n\nSimply put, this just means you need to provide a constraint for each ChoiceSite you encounter.\n\n\n\n\n\n","category":"function"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"UpdateContext\nupdate","category":"page"},{"location":"library_api/contexts/#Jaynes.UpdateContext","page":"Execution contexts","title":"Jaynes.UpdateContext","text":"mutable struct UpdateContext{C <: CallSite, \n                             T <: AddressMap,\n                             K <: AddressMap, \n                             P <: AddressMap, \n                             D <: Diff} <: ExecutionContext\n    prev::C\n    tr::T\n    select::K\n    weight::Float64\n    score::Float64\n    discard::DynamicAddressMap\n    visited::Visitor\n    params::P\n    argdiffs::D\nend\n\nInner constructor:\n\nUpdateContext(cl::C, select::K, argdiffs::D) where {C <: CallSite, K <: AddressMap, D <: Diff} = new{C, typeof(cl.trace), K, EmptyAddressMap, D}(cl, typeof(cl.trace)(), select, 0.0, 0.0, AddressMap(), Visitor(), AddressMap(), argdiffs)\nUpdateContext(cl::C, select::K, ps::P, argdiffs::D) where {C <: CallSite, K <: AddressMap, P <: AddressMap, D <: Diff} = new{C, typeof(cl.trace), K, EmptyAddressMap, D}(cl, typeof(cl.trace)(), select, 0.0, 0.0, AddressMap(), Visitor(), ps, argdiffs)\n\nUpdateContext is an execution context used for updating the value of random choices in an existing recorded call site. This context will perform corrective updates to the likehood weights and scores so that this operation produces the correct weights and scores for the original model program constrained with the select selection in the UpdateContext.\n\n\n\n\n\n","category":"type"},{"location":"library_api/contexts/#Jaynes.update","page":"Execution contexts","title":"Jaynes.update","text":"ret, cl, w, retdiff, d = update(ctx::UpdateContext, cs::DynamicCallSite, args...) where D <: Diff\nret, cl, w, retdiff, d = update(sel::L, cs::DynamicCallSite) where L <: AddressMap\nret, cl, w, retdiff, d = update(sel::L, cs::DynamicCallSite, argdiffs::D, new_args...) where {L <: AddressMap, D <: Diff}\nret, v_cl, w, retdiff, d = update(sel::L, vcs::VectorizedCallSite{typeof(plate)}) where {L <: AddressMap, D <: Diff}\nret, v_cl, w, retdiff, d = update(sel::L, vcs::VectorizedCallSite{typeof(markov)}) where {L <: AddressMap, D <: Diff}\nret, v_cl, w, retdiff, d = update(sel::L, vcs::VectorizedCallSite{typeof(markov)}, d::NoChange, len::Int) where {L <: AddressMap, D <: Diff}\nret, v_cl, w, retdiff, d = update(sel::L, vcs::VectorizedCallSite{typeof(markov)}, len::Int) where {L <: AddressMap, D <: Diff}\n\nupdate provides an API to the UpdateContext execution context. You can use this function on any of the matching signatures above - it will return the return value ret, the updated RecordSite instance cl or v_cl, the updated weight w, a Diff instance for the return value retdiff, and a structure which contains any changed (i.e. discarded) record sites d.\n\n\n\n\n\n","category":"function"},{"location":"library_api/contexts/","page":"Execution contexts","title":"Execution contexts","text":"RegenerateContext\nregenerate","category":"page"},{"location":"library_api/contexts/#Jaynes.RegenerateContext","page":"Execution contexts","title":"Jaynes.RegenerateContext","text":"mutable struct RegenerateContext{T <: Trace, \n                                 L <: Target,\n                                 P <: AddressMap} <: ExecutionContext\n    prev::T\n    tr::T\n    target::L\n    weight::Float64\n    score::Float64\n    discard::T\n    visited::Visitor\n    params::P\nend\n\nInner constructors:\n\nfunction RegenerateContext(tr::T, sel::Vector{Address}) where T <: Trace\n    un_sel = targetion(sel)\n    new{T, typeof(un_sel), EmptyAddressMap}(tr, Trace(), un_sel, 0.0, Trace(), Visitor(), AddressMap())\nend\nfunction RegenerateContext(tr::T, sel::L) where {T <: Trace, L <: Target}\n    new{T, L, EmptyAddressMap}(tr, Trace(), sel, 0.0, Trace(), Visitor(), AddressMap())\nend\n\nOuter constructors:\n\nRegenerate(tr::Trace, sel::Vector{Address}) = RegenerateContext(tr, sel)\nRegenerate(tr::Trace, sel::Target) = RegenerateContext(tr, sel)\n\nThe RegenerateContext is used for MCMC algorithms, to propose new choices for addresses indicated by an Target in the target field.\n\n\n\n\n\n","category":"type"},{"location":"library_api/contexts/#Jaynes.regenerate","page":"Execution contexts","title":"Jaynes.regenerate","text":"ret, cl = regenerate(sel::L, cs::DynamicCallSite, new_args...) where L <: Target\nret, cl = regenerate(sel::L, cs::DynamicCallSite) where L <: Target\n\nregenerate is an API to the RegenerateContext execution context. regenerate requires that users provide an Target, an original call site, and possibly a set of new arguments to be used in the regeneration step. This context internally keeps track of the bookkeeping required to increment likelihood weights, as well as prune off parts of the trace which are invalid if a regenerated choice changes the shape of the trace (e.g. control flow), and returns a new return value ret as well as the modified call site cl.\n\n\n\n\n\n","category":"function"},{"location":"inference/mh/","page":"Metropolis-Hastings","title":"Metropolis-Hastings","text":"CurrentModule = Jaynes","category":"page"},{"location":"inference/mh/","page":"Metropolis-Hastings","title":"Metropolis-Hastings","text":"metropolis_hastings","category":"page"},{"location":"inference/mh/#Jaynes.metropolis_hastings","page":"Metropolis-Hastings","title":"Jaynes.metropolis_hastings","text":"call, accepted, metropolis_hastings(sel::UnconstrainedSelection,\n                                    call::HierarchicalCallSite)\n\nPerform a Metropolis-Hastings step by proposing new choices using the prior at addressed specified by sel. Returns a call site, as well as a Boolean value accepted to indicate if the proposal was accepted or rejected.\n\ncall, accepted = metropolis_hastings(sel::UnconstrainedSelection,\n                                     call::HierarchicalCallSite,\n                                     proposal::Function,\n                                     proposal_args::Tuple)\n\nPerform a Metropolis-Hastings step by proposing new choices using a custom proposal at addressed specified by sel. Returns a call site, as well as a Boolean value accepted to indicate if the proposal was accepted or rejected.\n\n\n\n\n\n","category":"function"},{"location":"inference/mh/","page":"Metropolis-Hastings","title":"Metropolis-Hastings","text":"info: Info\nSimilar to custom proposals for particle filtering, a custom proposal passed to metropolis_hastings should accept an instance of CallSite as the first argument, followed by the other proposal arguments.Additionally, if the proposal proposes to addresses which modify the control flow of the original call, it must also provide proposal choice sites for any addressed which exist on that branch of the original model. If this criterion is not satisfied, the kernel is not stationary with respect to the target posterior of the model.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This is the documentation for the library API of the Jaynes probabilistic programming system.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Bon appetit!","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"info: Info\nThis library used to be called Walkman.jl because the original implementation used a beautiful compiler metaprogramming package called Cassette.jl, hence the punny name and colored cartoon Walkman logo. The original logo artwork came from an interesting article about the late and great musical device.","category":"page"},{"location":"library_api/fmi/","page":"Foreign model interface","title":"Foreign model interface","text":"CurrentModule = Jaynes","category":"page"},{"location":"library_api/fmi/","page":"Foreign model interface","title":"Foreign model interface","text":"Due to the design and implementation as an IR metaprogramming tool, Jaynes sits at a slightly privileged place in the probabilistic programming ecosystem, in the sense that many of the other languages which users are likely to use require the usage of macros to setup code in a way which allows the necessary state to be inserted for probabilistic programming functionality.","category":"page"},{"location":"library_api/fmi/","page":"Foreign model interface","title":"Foreign model interface","text":"Jaynes sees all the code after macro expansion is completed, which allows Jaynes to introspect function call sites after state has been inserted by other libraries. This allows the possibility for Jaynes to construct special call sites to represent calls into other probabilistic programming libraries. These interfaces are a work in progress, but Jaynes should theoretically provide a lingua franca for programs expressed in different probabilistic programming systems to communicate in a natural way, due to the nature of the context-oriented programming style facilitated by the system.","category":"page"},{"location":"library_api/fmi/#Black-box-extensions","page":"Foreign model interface","title":"Black-box extensions","text":"","category":"section"},{"location":"library_api/fmi/","page":"Foreign model interface","title":"Foreign model interface","text":"primitive","category":"page"},{"location":"library_api/fmi/#Jaynes.primitive","page":"Foreign model interface","title":"Jaynes.primitive","text":"@primitive function logpdf(fn::typeof(foo), args, foo_ret)\n    ...\nend\n\n@primitive is a convenience metaprogramming construct which derives contextual dispatch definitions for functions which you'd like the tracer to \"summarize\" to a single site. This is one mechanism which gives the user more control over the structure of the choice map structure in their programs.\n\nExample:\n\ngeo(p::Float64) = rand(:flip, Bernoulli(p)) ? 1 : 1 + rand(:geo, geo, p)\n\n# Define as primitive.\n@primitive function logpdf(fn::typeof(geo), p, count)\n    return Distributions.logpdf(Geometric(p), count)\nend\n\nfunction foo()\n    ret = rand(:geo, geo, 0.3)\n    ret\nend\n\nThe above program would summarize the trace into a single choice site for :geo, as if geo was a primitive distribution.\n\n  __________________________________\n\n               Addresses\n\n geo : 42\n  __________________________________\n\n\n\n\n\n","category":"function"},{"location":"library_api/fmi/#Foreign-models","page":"Foreign model interface","title":"Foreign models","text":"","category":"section"},{"location":"library_api/fmi/","page":"Foreign model interface","title":"Foreign model interface","text":"foreign\nload_soss_fmi\nload_gen_fmi","category":"page"},{"location":"library_api/fmi/#Jaynes.foreign","page":"Foreign model interface","title":"Jaynes.foreign","text":"foreign(addr::A, m, args...) where A <: Address\n\nActivate a foreign model interface. The tracer will treat this is a specialized call site, depending on the type of m. Currently supports typeof(m) <: Soss.Model and typeof(m) <: Gen.GenerativeFunction.\n\n\n\n\n\n","category":"function"},{"location":"library_api/fmi/#Jaynes.load_soss_fmi","page":"Foreign model interface","title":"Jaynes.load_soss_fmi","text":"Jaynes.@load_soss_fmi\n\n@load_soss_fmi loads the Soss.jl foreign model interface extension. This allows you to utilize Soss models in your modeling.\n\nExample:\n\nJaynes.@load_soss_fmi()\n\n# A Soss model.\nm = @model σ begin\n    μ ~ Normal()\n    y ~ Normal(μ, σ) |> iid(5)\nend\n\nbar = () -> begin\n    x = rand(:x, Normal(5.0, 1.0))\n    soss_ret = foreign(:foo, m, (σ = x,))\n    return soss_ret\nend\n\nThis interface currently supports all the inference interfaces (e.g. simulate, generate, score, regenerate, update, propose) which means that you can use any of the inference algorithms in the standard inference library.\n\n\n\n\n\n","category":"function"},{"location":"library_api/fmi/#Jaynes.load_gen_fmi","page":"Foreign model interface","title":"Jaynes.load_gen_fmi","text":"Jaynes.@load_gen_fmi\n\n@load_gen_fmi loads the Gen.jl foreign model interface extension. This allows you to utilize Gen models (in any of Gen's DSLs) in your modeling.\n\nExample:\n\nJaynes.@load_gen_fmi()\n\n@gen (static) function foo(z::Float64)\n    x = @trace(normal(z, 1.0), :x)\n    y = @trace(normal(x, 1.0), :y)\n    return x\nend\n\nGen.load_generated_functions()\n\nbar = () -> begin\n    x = rand(:x, Normal(0.0, 1.0))\n    return foreign(:foo, foo, x)\nend\n\nret, cl = Jaynes.simulate(bar)\n\nThis interface currently supports all the inference interfaces (e.g. simulate, generate, score, regenerate, update, propose) which means that you can use any of the inference algorithms in the standard inference library.\n\n\n\n\n\n","category":"function"}]
}
